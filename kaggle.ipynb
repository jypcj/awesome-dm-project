{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn\nimport math\nfrom torch.nn.parameter import Parameter\nimport numpy as np\n\n\n# node-level adaption model\nclass GraphNeuralNode(torch.nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(GraphNeuralNode, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n\n        if bias:\n            self.bias = Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n\n        stdv = 1. / math.sqrt(self.weight.size(1))\n\n        self.weight.data.uniform_(-stdv, stdv)\n\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, adj):\n\n        support = torch.mm(input, self.weight)\n        output = torch.spmm(adj, support)\n        if self.bias is not None:\n            return output + self.bias\n        else:\n            return output\n\n\n# used in node-level adaption\nclass GNN4NodeLevel(torch.nn.Module):\n    # 构建模型\n    def __init__(self, nfeat, nhid, dropout):\n        super(GNN4NodeLevel, self).__init__()\n\n        self.gc1 = GraphNeuralNode(nfeat, nhid)\n        self.gc2 = GraphNeuralNode(nhid, nhid)\n        self.dropout = dropout\n\n    # 前向传播\n    def forward(self, x, adj):\n        return self.gc1(x, adj)\n\n\n# node-level adaption model\nclass GraphNeuralClass(torch.nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(GraphNeuralClass, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n\n        if bias:\n            self.bias = Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n\n        self.weight.data.uniform_(-stdv, stdv)\n\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, adj, w, b):\n        if w is None and b is None:\n            alpha_w = alpha_b = beta_w = beta_b = 0\n        else:\n            alpha_w = w[0]\n            beta_w = w[1]\n            alpha_b = b[0]\n            beta_b = b[1]\n\n        support = torch.mm(input, self.weight * (1 + alpha_w) + beta_w)  # formula (6)\n        output = torch.mm(adj, support)\n\n        if self.bias is not None:\n            return output + self.bias * (1 + alpha_b) + beta_b\n        else:\n            return output\n\n\n# used in node-level adaption\nclass GNN4ClassLevel(torch.nn.Module):\n    def __init__(self, nfeat, nhid, dropout):\n        super(GNN4ClassLevel, self).__init__()\n\n        self.gc1 = GraphNeuralClass(nfeat, nhid)\n        self.gc2 = GraphNeuralClass(nhid, nhid)\n        self.generater = torch.nn.Linear(nfeat, (nfeat + 1) * nhid * 2 + (nhid + 1) * nhid * 2)\n\n        self.dropout = dropout\n\n    def permute(self, input_adj, input_feat, drop_rate=0.1):\n        # return input_adj\n\n        adj_random = torch.rand(input_adj.shape).cuda() + torch.eye(input_adj.shape[0]).cuda()\n\n        feat_random = np.random.choice(input_feat.shape[0], int(input_feat.shape[0] * drop_rate),\n                                       replace=False).tolist()\n\n        masks = torch.zeros(input_feat.shape).cuda()\n        masks[feat_random] = 1\n\n        random_tensor = torch.rand(input_feat.shape).cuda()\n\n        return input_adj * (adj_random > drop_rate), input_feat * (1 - masks) + random_tensor * masks\n\n    def forward(self, x, adj, w1=None, b1=None, w2=None, b2=None):\n        x = torch.nn.functional.relu(self.gc1(x, adj, w1, b1))\n        x = torch.nn.functional.dropout(x, self.dropout, training=self.training)\n        x = self.gc2(x, adj, w2, b2)\n        return x\n\n\n# used in classifier\nclass Linear(torch.nn.Module):\n    def __init__(self, in_features, out_features):\n        super(Linear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n        self.bias = Parameter(torch.FloatTensor(out_features))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n\n        self.weight.data.uniform_(-stdv, stdv)\n\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, w=None, b=None):\n        if w is not None:\n            return torch.mm(input, w) + b\n        else:\n            return torch.mm(input, self.weight) + self.bias\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-14T08:17:39.419927Z","iopub.execute_input":"2022-11-14T08:17:39.420385Z","iopub.status.idle":"2022-11-14T08:17:41.348696Z","shell.execute_reply.started":"2022-11-14T08:17:39.420301Z","shell.execute_reply":"2022-11-14T08:17:41.347760Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport scipy.sparse as sp\nimport torch\n\n\ndef normalize(mx):\n    \"\"\"Row-normalize sparse matrix\"\"\"\n    rowsum = np.array(mx.sum(1))\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    mx = r_mat_inv.dot(mx)\n    return mx\n\n\ndef sparse_matrix2torch_sparse_tensor(sparse_mx):\n    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\n\n        Parameter:\n            sparse_matrix: the scipy matrix to be conversed\n\n        Return:\n            torch sparse tensor\n    \"\"\"\n    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n    indices = torch.from_numpy(\n        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n    values = torch.from_numpy(sparse_mx.data)\n    shape = torch.Size(sparse_mx.shape)\n    return torch.sparse.FloatTensor(indices, values, shape)\n\n\ndef l2_normalize(x):\n    norm = x.pow(2).sum(1, keepdim=True).pow(1. / 2)\n    out = x.div(norm)\n    return out\n\n\ndef accuracy(output, labels):\n    preds = output.max(1)[1].type_as(labels)\n    correct = preds.eq(labels).double()\n    correct = correct.sum()\n    return correct / len(labels)\n\n\ndef InforNCE_Loss(anchor, sample, tau, dataset, all_negative=False, temperature_matrix=None):\n    def _similarity(h1: torch.Tensor, h2: torch.Tensor):\n        h1 = torch.nn.functional.normalize(h1)\n        h2 = torch.nn.functional.normalize(h2)\n        return h1 @ h2.t()\n\n    assert anchor.shape[0] == sample.shape[0]\n\n    pos_mask = torch.eye(anchor.shape[0], dtype=torch.float)\n\n    pos_mask = pos_mask.cuda()\n\n    neg_mask = 1. - pos_mask\n\n    sim = _similarity(anchor, sample / temperature_matrix if temperature_matrix != None else sample) / tau\n    exp_sim = torch.exp(sim) * (pos_mask + neg_mask)\n\n    if not all_negative:\n        log_prob = sim - torch.log(exp_sim.sum(dim=1, keepdim=True))\n    else:\n        log_prob = - torch.log(exp_sim.sum(dim=1, keepdim=True))\n\n    loss = log_prob * pos_mask\n    loss = loss.sum(dim=1) / pos_mask.sum(dim=1)\n\n    return -loss.mean(), sim\n\ndef sparse_mx_to_torch_sparse_tensor(sparse_mx):\n    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n    indices = torch.from_numpy(\n        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n    values = torch.from_numpy(sparse_mx.data)\n    shape = torch.Size(sparse_mx.shape)\n    return torch.sparse.FloatTensor(indices, values, shape)\n\ndef load_npz_to_sparse_graph(file_name):\n    \"\"\"Load a SparseGraph from a Numpy binary file.\n    Parameters\n    ----------\n    file_name : str\n        Name of the file to load.\n    Returns\n    -------\n    sparse_graph : SparseGraph\n        Graph in sparse matrix format.\n    \"\"\"\n    with np.load(file_name) as loader:\n        loader = dict(loader)\n        adj_matrix = sp.csr_matrix((loader['adj_data'], loader['adj_indices'], loader['adj_indptr']),\n                                   shape=loader['adj_shape'])\n\n        if 'attr_data' in loader:\n            # Attributes are stored as a sparse CSR matrix\n            attr_matrix = sp.csr_matrix((loader['attr_data'], loader['attr_indices'], loader['attr_indptr']),\n                                        shape=loader['attr_shape'])\n        elif 'attr_matrix' in loader:\n            # Attributes are stored as a (dense) np.ndarray\n            attr_matrix = loader['attr_matrix']\n        else:\n            attr_matrix = None\n\n        if 'labels_data' in loader:\n            # Labels are stored as a CSR matrix\n            labels = sp.csr_matrix((loader['labels_data'], loader['labels_indices'], loader['labels_indptr']),\n                                   shape=loader['labels_shape'])\n        elif 'labels' in loader:\n            # Labels are stored as a numpy array\n            labels = loader['labels']\n        else:\n            labels = None\n\n    return adj_matrix, attr_matrix, labels","metadata":{"execution":{"iopub.status.busy":"2022-11-14T08:17:44.160343Z","iopub.execute_input":"2022-11-14T08:17:44.160922Z","iopub.status.idle":"2022-11-14T08:17:44.214978Z","shell.execute_reply.started":"2022-11-14T08:17:44.160883Z","shell.execute_reply":"2022-11-14T08:17:44.214007Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import json\nfrom collections import defaultdict\n\nimport numpy\nimport scipy.io as sio\nimport scipy.sparse\nimport scipy.sparse as sp\nimport numpy as np\nimport torch\nfrom sklearn import preprocessing\nfrom scipy.sparse import coo_matrix\nfrom numpy import ndarray\n\n\n\n'''\n                           _ooOoo_\n                          o8888888o\n                          88\" . \"88\n                          (| -_- |)\n                          O\\  =  /O\n                       ____/`---'\\____\n                     .'  \\\\|     |//  `.\n                    /  \\\\|||  :  |||//  \\\n                   /  _||||| -:- |||||-  \\\n                   |   | \\\\\\  -  /// |   |\n                   | \\_|  ''\\---/''  |   |\n                   \\  .-\\__  `-`  ___/-. /\n                 ___`. .'  /--.--\\  `. . __\n              .\"\" '<  `.___\\_<|>_/___.'  >'\"\".\n             | | :  `- \\`.;`\\ _ /`;.`/ - ` : | |\n             \\  \\ `-.   \\_ __\\ /__ _/   .-` /  /\n        ======`-.____`-.___\\_____/___.-`____.-'======\n                           `=---='\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                 佛祖保佑       永无BUG\n'''\n\n\n# the name of dataset\ndatasets = ['Amazon_eletronics', 'dblp']\nCORA_FULL = 'cora-full'\nAMAZON_ELECTRONICS = 'Amazon_eletronics'\nDBLP = 'dblp'\nCORA_FULL_NPZ = \"cora_full.npz\"\n\n# directory of dataset\ndataset_dir = \"../input/dataset/dataset/\"\n\nclass Graph:\n    \"\"\" class Graph is used to store the information of a graph,\n        including adjacency_matrix, features_matrix, and etc.\n\n    \"\"\"\n\n    def __init__(self, adjacency_matrix, features_matrix, labels,\n                 train_node_index, valid_node_index, test_node_index,\n                 class_train_dict, class_valid_dict, class_test_dict):\n\n        self.adjacency_matrix: coo_matrix = adjacency_matrix\n        self.features_matrix: ndarray = features_matrix\n        self.labels: ndarray = labels\n\n        self.train_node_index: list = train_node_index\n        self.valid_node_index: list = valid_node_index\n        self.test_node_index: list = test_node_index\n\n        self.class_train_dict: defaultdict = class_train_dict\n        self.class_valid_dict: defaultdict = class_valid_dict\n        self.class_test_dict: defaultdict = class_test_dict\n\n\ndef data_preprocess(dataset: str) -> Graph:\n    \"\"\"pre-process the data before training\n\n        Parameter:\n            dataset: the name of dataset, including 'Amazon_eletronics', 'dblp', 'cora-full' and 'ogbn-arxiv'\n        Return:\n            graph: an object of Graph class, storing a graph data\n    \"\"\"\n    # [class0_id, class1_id, class2_id, ...]\n    train_class_list: list = list()\n    test_class_list: list = list()\n    valid_class_list: list = list()\n    train_class_list, valid_class_list, test_class_list = json.load(\n        open(dataset_dir + '{}_class_split.json'.format(dataset)))\n    if dataset == \"Amazon_eletronics\" or dataset == 'dblp':\n        # all the edges in graph is denoted by (node1[i], node2[i])\n        node1: list = list()\n        node2: list = list()\n        for line in open(dataset_dir + \"{}_network\".format(dataset)):\n            n1, n2 = line.strip().split(\"\\t\")\n            node1.append(int(n1))\n            node2.append(int(n2))\n        node_number: int = max(max(node1), max(node2)) + 1\n\n        # data_train and data_test are dicts, they have useful keys 'Index', 'Label' and 'Attributes'\n        # Index: [[1,2,3...]]\n        # Label: [[1],[1],[2],[2],...]\n        # Attributes: matrix\n        data_train: dict = sio.loadmat(dataset_dir + \"{}_train.mat\".format(dataset))\n        data_test: dict = sio.loadmat(dataset_dir + \"{}_test.mat\".format(dataset))\n        # label of nodes\n        labels = np.zeros((node_number, 1))\n        labels[data_train['Index']] = data_train[\"Label\"]\n        labels[data_test['Index']] = data_test[\"Label\"]\n        # print(labels)\n        # feature matrix\n        features_matrix = np.zeros((node_number, data_train[\"Attributes\"].shape[1]))\n        features_matrix[data_train['Index']] = data_train[\"Attributes\"].toarray()\n        features_matrix[data_test['Index']] = data_test[\"Attributes\"].toarray()\n        # adjacency matrix\n        adjacency_matrix = sp.coo_matrix((np.ones(len(node1)), (node1, node2)), shape=(node_number, node_number))\n\n        # all the classes in a list\n        all_class_list: list = []\n        for cls in labels:\n            if cls[0] not in all_class_list:\n                all_class_list.append(cls[0])\n\n        # {class_id -> [node_id, node_id, ...]}\n        class_dict: dict = {}\n        for cls in all_class_list:\n            class_dict[cls] = []\n        for node_id, class_id in enumerate(labels):\n            class_dict[class_id[0]].append(node_id)\n\n        label_binarizer = preprocessing.LabelBinarizer()\n        labels = label_binarizer.fit_transform(labels)\n        features_matrix = torch.FloatTensor(features_matrix)\n        # labels = tensor([99, 61, 99, ..., 57, 97, 34])\n        labels = torch.LongTensor(np.where(labels)[1])\n        adjacency_matrix = sparse_matrix2torch_sparse_tensor(\n            normalize(adjacency_matrix + sp.eye(adjacency_matrix.shape[0])))\n        # print(labels)\n    \n    elif dataset == 'cora-full':\n        adjacency_matrix, features_matrix, labels = load_npz_to_sparse_graph(dataset_dir + CORA_FULL_NPZ)\n\n        adjacency_matrix = normalize(adjacency_matrix.tocoo() + sp.eye(adjacency_matrix.shape[0]))\n        adjacency_matrix = sparse_mx_to_torch_sparse_tensor(adjacency_matrix)\n        features_matrix = features_matrix.todense()\n        features_matrix = torch.FloatTensor(features_matrix)\n        labels = torch.LongTensor(labels).squeeze()\n\n        all_class_list = train_class_list + valid_class_list + test_class_list\n\n        class_dict: dict = {}\n        for i in all_class_list:\n            class_dict[i] = []\n        for node_id, cls in enumerate(labels.numpy().tolist()):\n            class_dict[cls].append(node_id)\n    \n    # store node id\n    train_node_index: list = list()\n    valid_node_index: list = list()\n    test_node_index: list = list()\n    for idx, class_list in zip([train_node_index, valid_node_index, test_node_index],\n                               [train_class_list, valid_class_list, test_class_list]):\n        for class_id in class_list:\n            idx.extend(class_dict[class_id])\n\n    # {class_id => [node0_id, node1_id, ...]}\n    class_train_dict = defaultdict(list)\n    for one in train_class_list:\n        for i, label in enumerate(labels.numpy().tolist()):\n            if label == one:\n                class_train_dict[one].append(i)\n    class_valid_dict = defaultdict(list)\n    for one in valid_class_list:\n        for i, label in enumerate(labels.numpy().tolist()):\n            if label == one:\n                class_valid_dict[one].append(i)\n\n    class_test_dict = defaultdict(list)\n    for one in test_class_list:\n        for i, label in enumerate(labels.numpy().tolist()):\n            if label == one:\n                class_test_dict[one].append(i)\n\n    graph = Graph(adjacency_matrix, features_matrix, labels,\n                  train_node_index, valid_node_index, test_node_index,\n                  class_train_dict, class_valid_dict, class_test_dict)\n\n    return graph\n\n\n\n\ndef print_content_to_file(data, file_name):\n    \"\"\" This function is used for debugging\n\n    \"\"\"\n    with open(\"./debug/\" + file_name, 'w') as f:\n        f.write(data)\n        \n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-14T08:17:49.588632Z","iopub.execute_input":"2022-11-14T08:17:49.589099Z","iopub.status.idle":"2022-11-14T08:17:49.981361Z","shell.execute_reply.started":"2022-11-14T08:17:49.589061Z","shell.execute_reply":"2022-11-14T08:17:49.980350Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport json\nimport random\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nfrom torch import Tensor, optim\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\n\n\n# the name of dataset\ndatasets = ['cora-full','dblp','Amazon_eletronics']\nCORA_FULL = 'cora-full'\nAMAZON_ELECTRONICS = 'Amazon_eletronics'\nDBLP = 'dblp'\n\n# the name of mode\nTRAIN = 'train'\nVALID = 'valid'\nTEST = 'test'\n\n\n\n# K and N in experiment\n# k nodes for each of n classes\nKs: list = [3, 5]\nNs: list = [5, 10]\n\nquery_size = 10\n\n# repeat times for each (n, k)\nrepeat_times = 5\n\nfinal_results = defaultdict(dict)\n\n# loss function\nloss_function = torch.nn.CrossEntropyLoss()\n\n# parameters\nclass args:\n    use_cuda = True\n    seed = 1234\n    epochs = 2000\n    test_epochs = 100\n    lr = 0.05\n    weight_decay = 5e-4\n    hidden1 = 16\n    hidden2 = 16\n    dropout = 0.2\n\n\ndef main():\n    \"\"\"main function of the whole project\n    \"\"\"\n\n    # parameter\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.use_cuda:\n        torch.cuda.manual_seed(args.seed)\n\n    train_and_test()\n\n\ndef train_and_test():\n    # conduct the experiment in each dataset\n    for dataset in datasets:\n        graph = data_preprocess(dataset)\n\n        adj_dense = graph.adjacency_matrix.to_dense()\n        adj_dense = adj_dense.cuda()\n\n        for n in Ns:\n            for k in Ks:\n                for repeat in range(repeat_times):\n                    print(\"begin \", dataset, \"n= \", n, \"k= \", k)\n\n                    # two models in class-level and node level adaption\n                    node_level_model: GNN4NodeLevel = GNN4NodeLevel(nfeat=graph.features_matrix.shape[1],\n                                                                    nhid=args.hidden1,\n                                                                    dropout=args.dropout)\n                    class_level_model: GNN4ClassLevel = GNN4ClassLevel(nfeat=args.hidden1,\n                                                                       nhid=args.hidden2,\n                                                                       dropout=args.dropout)\n                    support_labels: Tensor = torch.zeros(n * k, dtype=torch.long)\n                    query_labels: Tensor = torch.zeros(n * k, dtype=torch.long)\n\n                    for i in range(n):\n                        support_labels[i * k:(i + 1) * k] = i\n                    for i in range(n):\n                        query_labels[i * query_size:(i + 1) * query_size] = i\n\n                    classifier: Linear = Linear(args.hidden1, graph.labels.max().item() + 1)\n                    optimizer: optim.Adam = optim.Adam(\n                        [{'params': class_level_model.parameters()},\n                         {'params': classifier.parameters()},\n                         {'params': node_level_model.parameters()}],\n                        lr=args.lr, weight_decay=args.weight_decay)\n\n                    if args.use_cuda:\n                        node_level_model = node_level_model.cuda()\n                        class_level_model = class_level_model.cuda()\n\n                        graph.features_matrix = graph.features_matrix.cuda()\n                        graph.adjacency_matrix = graph.adjacency_matrix.cuda()\n                        graph.labels = graph.labels.cuda()\n\n                        classifier = classifier.cuda()\n\n                        support_labels = support_labels.cuda()\n                        query_labels = query_labels.cuda()\n\n                    def calculate_accuracy(epoch: int,\n                                           n: int, k: int,\n                                           mode: str) -> float:\n                        if mode == 'train':\n                            class_level_model.train()\n                            optimizer.zero_grad()\n                        else:\n                            class_level_model.eval()\n\n                            # first-step node representation?\n                        emb_features = node_level_model(graph.features_matrix, graph.adjacency_matrix)\n\n                        target_idx = []\n                        target_graph_adj_and_feat = []\n                        support_graph_adj_and_feat = []\n\n                        pos_node_idx = []\n\n                        if mode == 'train':\n                            class_dict = graph.class_train_dict\n                        elif mode == 'test':\n                            class_dict = graph.class_test_dict\n                        elif mode == 'valid':\n                            class_dict = graph.class_valid_dict\n\n                        K = k\n                        N = n\n                        Q = query_size\n\n                        classes = np.random.choice(list(class_dict.keys()), N, replace=False).tolist()\n\n                        pos_graph_adj_and_feat = []\n                        # construct class-ego subgraphs?\n                        for i in classes:\n                            # sample from one specific class\n                            sampled_idx = np.random.choice(class_dict[i], K + Q, replace=False).tolist()\n                            pos_node_idx.extend(sampled_idx[:K])\n                            target_idx.extend(sampled_idx[K:])\n\n                            class_pos_idx = sampled_idx[:K]\n\n                            # why k = 1?\n                            if K == 1 and torch.nonzero(adj_dense[class_pos_idx, :]).shape[0] == 1:\n                                pos_class_graph_adj = adj_dense[class_pos_idx, class_pos_idx].reshape([1, 1])\n                                pos_graph_feat = emb_features[class_pos_idx]\n                            else:\n                                pos_graph_neighbors = torch.nonzero(adj_dense[class_pos_idx, :].sum(0)).squeeze()\n\n                                pos_graph_adj = adj_dense[pos_graph_neighbors, :][:, pos_graph_neighbors]\n\n                                pos_class_graph_adj = torch.eye(pos_graph_neighbors.shape[0] + 1, dtype=torch.float)\n\n                                pos_class_graph_adj[1:, 1:] = pos_graph_adj\n\n                                pos_graph_feat = torch.cat([emb_features[class_pos_idx].mean(0, keepdim=True),\n                                                            emb_features[pos_graph_neighbors]], 0)\n\n                            if dataset != 'ogbn-arxiv':\n                                pos_class_graph_adj = pos_class_graph_adj.cuda()\n\n                            pos_graph_adj_and_feat.append((pos_class_graph_adj, pos_graph_feat))\n\n                        target_graph_adj_and_feat = []\n                        for node in target_idx:\n                            if torch.nonzero(adj_dense[node, :]).shape[0] == 1:\n                                pos_graph_adj = adj_dense[node, node].reshape([1, 1])\n                                pos_graph_feat = emb_features[node].unsqueeze(0)\n                            else:\n                                pos_graph_neighbors = torch.nonzero(adj_dense[node, :]).squeeze()\n                                pos_graph_neighbors = torch.nonzero(adj_dense[pos_graph_neighbors, :].sum(0)).squeeze()\n                                pos_graph_adj = adj_dense[pos_graph_neighbors, :][:, pos_graph_neighbors]\n                                pos_graph_feat = emb_features[pos_graph_neighbors]\n\n                            target_graph_adj_and_feat.append((pos_graph_adj, pos_graph_feat))\n\n                        class_generate_emb = torch.stack([sub[1][0] for sub in pos_graph_adj_and_feat], 0).mean(0)\n\n                        parameters = class_level_model.generater(class_generate_emb)\n\n                        gc1_parameters = parameters[:(args.hidden1 + 1) * args.hidden2 * 2]\n                        gc2_parameters = parameters[(args.hidden1 + 1) * args.hidden2 * 2:]\n\n                        gc1_w = gc1_parameters[:args.hidden1 * args.hidden2 * 2].reshape(\n                            [2, args.hidden1, args.hidden2])\n                        gc1_b = gc1_parameters[args.hidden1 * args.hidden2 * 2:].reshape([2, args.hidden2])\n\n                        gc2_w = gc2_parameters[:args.hidden2 * args.hidden2 * 2].reshape(\n                            [2, args.hidden2, args.hidden2])\n                        gc2_b = gc2_parameters[args.hidden2 * args.hidden2 * 2:].reshape([2, args.hidden2])\n\n                        class_level_model.eval()\n                        ori_emb = []\n                        for i, one in enumerate(target_graph_adj_and_feat):\n                            sub_adj, sub_feat = one[0], one[1]\n                            ori_emb.append(class_level_model(sub_feat, sub_adj, gc1_w, gc1_b, gc2_w, gc2_b).mean(0))  # .mean(0))\n\n                        target_embs = torch.stack(ori_emb, 0)\n\n                        class_ego_embs = []\n                        for sub_adj, sub_feat in pos_graph_adj_and_feat:\n                            class_ego_embs.append(class_level_model(sub_feat, sub_adj, gc1_w, gc1_b, gc2_w, gc2_b)[0])\n                        class_ego_embs = torch.stack(class_ego_embs, 0)\n\n                        target_embs = target_embs.reshape([N, Q, -1]).transpose(0, 1)\n\n                        support_features = emb_features[pos_node_idx].reshape([N, K, -1])\n                        class_features = support_features.mean(1)\n                        taus = []\n                        for j in range(N):\n                            taus.append(torch.linalg.norm(support_features[j] - class_features[j], -1).sum(0))\n                        taus = torch.stack(taus, 0)\n\n                        similarities = []\n                        for j in range(Q):\n                            class_contras_loss, similarity = InforNCE_Loss(target_embs[j],\n                                                                           class_ego_embs / taus.unsqueeze(-1), tau=0.5,dataset=dataset)\n                            similarities.append(similarity)\n\n                        loss_supervised = loss_function(classifier(emb_features[graph.train_node_index]), graph.labels[graph.train_node_index])\n\n                        loss = loss_supervised\n\n                        labels_train = graph.labels[target_idx]\n                        for j, class_idx in enumerate(classes[:N]):\n                            labels_train[labels_train == class_idx] = j\n\n                        loss += loss_function(torch.stack(similarities, 0).transpose(0, 1).reshape([N * Q, -1]), labels_train)\n\n                        acc_train = accuracy(torch.stack(similarities, 0).transpose(0, 1).reshape([N * Q, -1]),\n                                             labels_train)\n\n                        if mode == 'valid' or mode == 'test' or (mode == 'train' and epoch % 250 == 249):\n                            support_features = l2_normalize(emb_features[pos_node_idx].detach().cpu()).numpy()\n                            query_features = l2_normalize(emb_features[target_idx].detach().cpu()).numpy()\n\n                            support_labels = torch.zeros(N * K, dtype=torch.long)\n                            for i in range(N):\n                                support_labels[i * K:(i + 1) * K] = i\n\n                            query_labels = torch.zeros(N * Q, dtype=torch.long)\n                            for i in range(N):\n                                query_labels[i * Q:(i + 1) * Q] = i\n\n                            clf = LogisticRegression(penalty='l2',\n                                                     random_state=0,\n                                                     C=1.0,\n                                                     solver='lbfgs',\n                                                     max_iter=1000,\n                                                     multi_class='multinomial')\n                            clf.fit(support_features, support_labels.numpy())\n                            query_ys_pred = clf.predict(query_features)\n\n                            acc_train = metrics.accuracy_score(query_labels, query_ys_pred)\n\n                        if mode == 'train':\n                            loss.backward()\n                            optimizer.step()\n\n                        if epoch % 250 == 249 and mode == 'train':\n                            print('Epoch: {:04d}'.format(epoch + 1),\n                                  'loss_train: {:.4f}'.format(loss.item()),\n                                  'acc_train: {:.4f}'.format(acc_train.item()))\n                        return acc_train.item()\n\n                    # begin to train and test\n                    cnt: int = 0\n                    valid_accuracy_best: float = 0.0\n                    test_accuracy_best: list = []\n                    for epoch in range(args.epochs):\n                        train_accuracy: float = calculate_accuracy(\n                                                                   epoch=epoch,\n                                                                   n=n, k=k,\n                                                                   mode=TRAIN)\n\n                        # epoch for test and valid\n                        if epoch % 50 == 0 and epoch != 0:\n                            tmp_accuracies: list = []\n                            for test_epoch in range(50):\n                                tmp_accuracy = calculate_accuracy(\n                                                                  epoch=test_epoch,\n                                                                  n=n, k=k,\n                                                                  mode=TEST)\n                                tmp_accuracies.append(tmp_accuracy)\n\n                            valid_accuracies: list = []\n                            for valid_epoch in range(50):\n                                tmp_accuracy = calculate_accuracy(\n                                                                  epoch=valid_epoch,\n                                                                  n=n, k=k,\n                                                                  mode=VALID)\n                                valid_accuracies.append(tmp_accuracy)\n\n                            valid_accuracy = np.array(valid_accuracies).mean(axis=0)\n\n                            print(\"Epoch: {:04d} Meta-valid_Accuracy: {:.4f}\".format(epoch + 1, valid_accuracy))\n\n                            if valid_accuracy > valid_accuracy_best:\n                                valid_accuracy_best = valid_accuracy\n                                test_accuracy_best = tmp_accuracies\n                                cnt = 0\n                            else:\n                                cnt += 1\n                                if cnt >= 10:\n                                    break\n\n                    print('Test Acc', np.array(test_accuracy_best).mean(axis=0))\n                    final_results[dataset]['{}-way {}-shot {}-repeat'.format(n, k, repeat)] = [\n                        np.array(test_accuracy_best).mean(axis=0)]\n                    json.dump(final_results[dataset], open('./TENT-result_{}.json'.format(dataset), 'w'))\n\n                final_accuracies: list = []\n                for i in range(repeat_times):\n                    final_accuracies.append(final_results[dataset]['{}-way {}-shot {}-repeat'.format(n, k, i)][0])\n\n                final_results[dataset]['{}-way {}-shot'.format(n, k)] = [np.mean(final_accuracies)]\n                final_results[dataset]['{}-way {}-shot_print'.format(n, k)] = 'acc: {:.4f}'.format(\n                    np.mean(final_accuracies))\n\n                json.dump(final_results[dataset], open('./TENT-result_{}.json'.format(dataset), 'w'))\n\n                del node_level_model\n                del class_level_model\n\n        del graph\n        del adj_dense\n\n\n\n\n\n# return 0.0\n\n\n# entry of the program\nif __name__ == '__main__':\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2022-11-14T08:17:53.316520Z","iopub.execute_input":"2022-11-14T08:17:53.316895Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"begin  cora-full n=  5 k=  3\nEpoch: 0051 Meta-valid_Accuracy: 0.6396\nEpoch: 0101 Meta-valid_Accuracy: 0.6464\nEpoch: 0151 Meta-valid_Accuracy: 0.6156\nEpoch: 0201 Meta-valid_Accuracy: 0.6528\nEpoch: 0250 loss_train: 0.9675 acc_train: 0.9400\nEpoch: 0251 Meta-valid_Accuracy: 0.6452\nEpoch: 0301 Meta-valid_Accuracy: 0.6448\nEpoch: 0351 Meta-valid_Accuracy: 0.6264\nEpoch: 0401 Meta-valid_Accuracy: 0.6464\nEpoch: 0451 Meta-valid_Accuracy: 0.6580\nEpoch: 0500 loss_train: 0.9370 acc_train: 0.9800\nEpoch: 0501 Meta-valid_Accuracy: 0.6632\nEpoch: 0551 Meta-valid_Accuracy: 0.6588\nEpoch: 0601 Meta-valid_Accuracy: 0.6948\nEpoch: 0651 Meta-valid_Accuracy: 0.6588\nEpoch: 0701 Meta-valid_Accuracy: 0.6700\nEpoch: 0750 loss_train: 1.1206 acc_train: 0.9400\nEpoch: 0751 Meta-valid_Accuracy: 0.6536\nEpoch: 0801 Meta-valid_Accuracy: 0.6820\nEpoch: 0851 Meta-valid_Accuracy: 0.6620\nEpoch: 0901 Meta-valid_Accuracy: 0.6560\nEpoch: 0951 Meta-valid_Accuracy: 0.6740\nEpoch: 1000 loss_train: 0.9437 acc_train: 0.9600\nEpoch: 1001 Meta-valid_Accuracy: 0.6984\nEpoch: 1051 Meta-valid_Accuracy: 0.6576\nEpoch: 1101 Meta-valid_Accuracy: 0.6600\nEpoch: 1151 Meta-valid_Accuracy: 0.6400\nEpoch: 1201 Meta-valid_Accuracy: 0.6632\nEpoch: 1250 loss_train: 1.1466 acc_train: 1.0000\nEpoch: 1251 Meta-valid_Accuracy: 0.6616\nEpoch: 1301 Meta-valid_Accuracy: 0.6936\nEpoch: 1351 Meta-valid_Accuracy: 0.6484\nEpoch: 1401 Meta-valid_Accuracy: 0.6580\nEpoch: 1451 Meta-valid_Accuracy: 0.7056\nEpoch: 1500 loss_train: 1.0264 acc_train: 0.9600\nEpoch: 1501 Meta-valid_Accuracy: 0.6552\nEpoch: 1551 Meta-valid_Accuracy: 0.7048\nEpoch: 1601 Meta-valid_Accuracy: 0.6496\nEpoch: 1651 Meta-valid_Accuracy: 0.6704\nEpoch: 1701 Meta-valid_Accuracy: 0.6612\nEpoch: 1750 loss_train: 1.1683 acc_train: 0.9800\nEpoch: 1751 Meta-valid_Accuracy: 0.6732\nEpoch: 1801 Meta-valid_Accuracy: 0.6728\nEpoch: 1851 Meta-valid_Accuracy: 0.6544\nEpoch: 1901 Meta-valid_Accuracy: 0.6768\nEpoch: 1951 Meta-valid_Accuracy: 0.6656\nTest Acc 0.6896\nbegin  cora-full n=  5 k=  3\nEpoch: 0051 Meta-valid_Accuracy: 0.6508\nEpoch: 0101 Meta-valid_Accuracy: 0.6472\nEpoch: 0151 Meta-valid_Accuracy: 0.6216\nEpoch: 0201 Meta-valid_Accuracy: 0.6304\nEpoch: 0250 loss_train: 1.3152 acc_train: 0.9400\nEpoch: 0251 Meta-valid_Accuracy: 0.6600\nEpoch: 0301 Meta-valid_Accuracy: 0.6400\nEpoch: 0351 Meta-valid_Accuracy: 0.6268\nEpoch: 0401 Meta-valid_Accuracy: 0.6472\nEpoch: 0451 Meta-valid_Accuracy: 0.6424\nEpoch: 0500 loss_train: 0.9457 acc_train: 1.0000\nEpoch: 0501 Meta-valid_Accuracy: 0.6756\nEpoch: 0551 Meta-valid_Accuracy: 0.6448\nEpoch: 0601 Meta-valid_Accuracy: 0.6668\nEpoch: 0651 Meta-valid_Accuracy: 0.6624\nEpoch: 0701 Meta-valid_Accuracy: 0.6496\nEpoch: 0750 loss_train: 0.9325 acc_train: 0.9800\nEpoch: 0751 Meta-valid_Accuracy: 0.6616\nEpoch: 0801 Meta-valid_Accuracy: 0.6532\nEpoch: 0851 Meta-valid_Accuracy: 0.6496\nEpoch: 0901 Meta-valid_Accuracy: 0.6672\nEpoch: 0951 Meta-valid_Accuracy: 0.6660\nEpoch: 1000 loss_train: 1.0140 acc_train: 0.9800\nEpoch: 1001 Meta-valid_Accuracy: 0.6308\nTest Acc 0.688\nbegin  cora-full n=  5 k=  3\nEpoch: 0051 Meta-valid_Accuracy: 0.6508\nEpoch: 0101 Meta-valid_Accuracy: 0.6632\nEpoch: 0151 Meta-valid_Accuracy: 0.6632\nEpoch: 0201 Meta-valid_Accuracy: 0.6828\nEpoch: 0250 loss_train: 0.9349 acc_train: 1.0000\nEpoch: 0251 Meta-valid_Accuracy: 0.6836\nEpoch: 0301 Meta-valid_Accuracy: 0.6704\nEpoch: 0351 Meta-valid_Accuracy: 0.6472\nEpoch: 0401 Meta-valid_Accuracy: 0.6532\nEpoch: 0451 Meta-valid_Accuracy: 0.6672\nEpoch: 0500 loss_train: 0.9022 acc_train: 0.9400\nEpoch: 0501 Meta-valid_Accuracy: 0.6660\nEpoch: 0551 Meta-valid_Accuracy: 0.6376\nEpoch: 0601 Meta-valid_Accuracy: 0.6864\nEpoch: 0651 Meta-valid_Accuracy: 0.6452\nEpoch: 0701 Meta-valid_Accuracy: 0.6464\nEpoch: 0750 loss_train: 0.9086 acc_train: 1.0000\nEpoch: 0751 Meta-valid_Accuracy: 0.6732\nEpoch: 0801 Meta-valid_Accuracy: 0.6616\nEpoch: 0851 Meta-valid_Accuracy: 0.6948\nEpoch: 0901 Meta-valid_Accuracy: 0.6728\nEpoch: 0951 Meta-valid_Accuracy: 0.6540\nEpoch: 1000 loss_train: 0.9881 acc_train: 0.9600\nEpoch: 1001 Meta-valid_Accuracy: 0.6764\nEpoch: 1051 Meta-valid_Accuracy: 0.6444\nEpoch: 1101 Meta-valid_Accuracy: 0.6636\nEpoch: 1151 Meta-valid_Accuracy: 0.6752\nEpoch: 1201 Meta-valid_Accuracy: 0.6544\nEpoch: 1250 loss_train: 0.8955 acc_train: 0.9600\nEpoch: 1251 Meta-valid_Accuracy: 0.6816\nEpoch: 1301 Meta-valid_Accuracy: 0.6788\nEpoch: 1351 Meta-valid_Accuracy: 0.6540\nTest Acc 0.6655999999999999\nbegin  cora-full n=  5 k=  3\nEpoch: 0051 Meta-valid_Accuracy: 0.6172\nEpoch: 0101 Meta-valid_Accuracy: 0.6328\nEpoch: 0151 Meta-valid_Accuracy: 0.6140\nEpoch: 0201 Meta-valid_Accuracy: 0.6412\nEpoch: 0250 loss_train: 1.0410 acc_train: 0.9400\nEpoch: 0251 Meta-valid_Accuracy: 0.6376\nEpoch: 0301 Meta-valid_Accuracy: 0.6460\nEpoch: 0351 Meta-valid_Accuracy: 0.6600\nEpoch: 0401 Meta-valid_Accuracy: 0.6560\nEpoch: 0451 Meta-valid_Accuracy: 0.6496\nEpoch: 0500 loss_train: 1.0003 acc_train: 0.9800\nEpoch: 0501 Meta-valid_Accuracy: 0.6744\nEpoch: 0551 Meta-valid_Accuracy: 0.6468\nEpoch: 0601 Meta-valid_Accuracy: 0.6916\nEpoch: 0651 Meta-valid_Accuracy: 0.6296\nEpoch: 0701 Meta-valid_Accuracy: 0.6684\nEpoch: 0750 loss_train: 0.9314 acc_train: 0.9600\nEpoch: 0751 Meta-valid_Accuracy: 0.6456\nEpoch: 0801 Meta-valid_Accuracy: 0.6360\nEpoch: 0851 Meta-valid_Accuracy: 0.6292\nEpoch: 0901 Meta-valid_Accuracy: 0.6260\nEpoch: 0951 Meta-valid_Accuracy: 0.6600\nEpoch: 1000 loss_train: 0.9601 acc_train: 0.9600\nEpoch: 1001 Meta-valid_Accuracy: 0.6584\nEpoch: 1051 Meta-valid_Accuracy: 0.6416\nEpoch: 1101 Meta-valid_Accuracy: 0.6380\nTest Acc 0.6300000000000001\nbegin  cora-full n=  5 k=  3\nEpoch: 0051 Meta-valid_Accuracy: 0.6540\nEpoch: 0101 Meta-valid_Accuracy: 0.6060\nEpoch: 0151 Meta-valid_Accuracy: 0.6216\nEpoch: 0201 Meta-valid_Accuracy: 0.6628\nEpoch: 0250 loss_train: 0.9330 acc_train: 0.9800\nEpoch: 0251 Meta-valid_Accuracy: 0.6612\nEpoch: 0301 Meta-valid_Accuracy: 0.6844\nEpoch: 0351 Meta-valid_Accuracy: 0.6556\nEpoch: 0401 Meta-valid_Accuracy: 0.6756\nEpoch: 0451 Meta-valid_Accuracy: 0.6680\nEpoch: 0500 loss_train: 1.0132 acc_train: 0.9400\nEpoch: 0501 Meta-valid_Accuracy: 0.6236\nEpoch: 0551 Meta-valid_Accuracy: 0.6536\nEpoch: 0601 Meta-valid_Accuracy: 0.6860\nEpoch: 0651 Meta-valid_Accuracy: 0.6508\nEpoch: 0701 Meta-valid_Accuracy: 0.6788\nEpoch: 0750 loss_train: 1.0307 acc_train: 0.9200\nEpoch: 0751 Meta-valid_Accuracy: 0.6428\nEpoch: 0801 Meta-valid_Accuracy: 0.6648\nEpoch: 0851 Meta-valid_Accuracy: 0.6568\nEpoch: 0901 Meta-valid_Accuracy: 0.6668\nEpoch: 0951 Meta-valid_Accuracy: 0.6364\nEpoch: 1000 loss_train: 0.9718 acc_train: 0.9200\nEpoch: 1001 Meta-valid_Accuracy: 0.6440\nEpoch: 1051 Meta-valid_Accuracy: 0.6620\nEpoch: 1101 Meta-valid_Accuracy: 0.6676\nTest Acc 0.6200000000000001\nbegin  cora-full n=  5 k=  5\nEpoch: 0051 Meta-valid_Accuracy: 0.7108\nEpoch: 0101 Meta-valid_Accuracy: 0.7176\nEpoch: 0151 Meta-valid_Accuracy: 0.7024\nEpoch: 0201 Meta-valid_Accuracy: 0.7028\nEpoch: 0250 loss_train: 0.9266 acc_train: 0.9800\nEpoch: 0251 Meta-valid_Accuracy: 0.7072\nEpoch: 0301 Meta-valid_Accuracy: 0.7212\nEpoch: 0351 Meta-valid_Accuracy: 0.7204\nEpoch: 0401 Meta-valid_Accuracy: 0.6904\nEpoch: 0451 Meta-valid_Accuracy: 0.6884\nEpoch: 0500 loss_train: 0.9740 acc_train: 0.9800\nEpoch: 0501 Meta-valid_Accuracy: 0.7128\nEpoch: 0551 Meta-valid_Accuracy: 0.6800\nEpoch: 0601 Meta-valid_Accuracy: 0.6904\nEpoch: 0651 Meta-valid_Accuracy: 0.7144\nEpoch: 0701 Meta-valid_Accuracy: 0.7036\nEpoch: 0750 loss_train: 0.9210 acc_train: 0.9800\nEpoch: 0751 Meta-valid_Accuracy: 0.7100\nEpoch: 0801 Meta-valid_Accuracy: 0.7400\nEpoch: 0851 Meta-valid_Accuracy: 0.7152\nEpoch: 0901 Meta-valid_Accuracy: 0.6940\nEpoch: 0951 Meta-valid_Accuracy: 0.7256\nEpoch: 1000 loss_train: 0.8317 acc_train: 0.9800\nEpoch: 1001 Meta-valid_Accuracy: 0.7048\nEpoch: 1051 Meta-valid_Accuracy: 0.6808\nEpoch: 1101 Meta-valid_Accuracy: 0.7188\nEpoch: 1151 Meta-valid_Accuracy: 0.7064\nEpoch: 1201 Meta-valid_Accuracy: 0.7192\nEpoch: 1250 loss_train: 1.2331 acc_train: 0.9800\nEpoch: 1251 Meta-valid_Accuracy: 0.7012\nEpoch: 1301 Meta-valid_Accuracy: 0.7200\nTest Acc 0.682\nbegin  cora-full n=  5 k=  5\nEpoch: 0051 Meta-valid_Accuracy: 0.6972\nEpoch: 0101 Meta-valid_Accuracy: 0.6988\nEpoch: 0151 Meta-valid_Accuracy: 0.6936\nEpoch: 0201 Meta-valid_Accuracy: 0.7028\nEpoch: 0250 loss_train: 1.0335 acc_train: 0.9400\nEpoch: 0251 Meta-valid_Accuracy: 0.7092\nEpoch: 0301 Meta-valid_Accuracy: 0.6968\nEpoch: 0351 Meta-valid_Accuracy: 0.7052\nEpoch: 0401 Meta-valid_Accuracy: 0.7152\nEpoch: 0451 Meta-valid_Accuracy: 0.6988\nEpoch: 0500 loss_train: 1.0033 acc_train: 0.9800\nEpoch: 0501 Meta-valid_Accuracy: 0.7032\nEpoch: 0551 Meta-valid_Accuracy: 0.7140\nEpoch: 0601 Meta-valid_Accuracy: 0.7240\nEpoch: 0651 Meta-valid_Accuracy: 0.7184\nEpoch: 0701 Meta-valid_Accuracy: 0.7004\nEpoch: 0750 loss_train: 1.4071 acc_train: 0.9800\nEpoch: 0751 Meta-valid_Accuracy: 0.6956\nEpoch: 0801 Meta-valid_Accuracy: 0.6820\nEpoch: 0851 Meta-valid_Accuracy: 0.7052\nEpoch: 0901 Meta-valid_Accuracy: 0.7104\nEpoch: 0951 Meta-valid_Accuracy: 0.7308\nEpoch: 1000 loss_train: 0.9595 acc_train: 0.9600\nEpoch: 1001 Meta-valid_Accuracy: 0.6932\nEpoch: 1051 Meta-valid_Accuracy: 0.7136\nEpoch: 1101 Meta-valid_Accuracy: 0.7092\nEpoch: 1151 Meta-valid_Accuracy: 0.6920\nEpoch: 1201 Meta-valid_Accuracy: 0.7076\nEpoch: 1250 loss_train: 1.0304 acc_train: 0.9800\nEpoch: 1251 Meta-valid_Accuracy: 0.6952\nEpoch: 1301 Meta-valid_Accuracy: 0.7460\nEpoch: 1351 Meta-valid_Accuracy: 0.6924\nEpoch: 1401 Meta-valid_Accuracy: 0.7332\nEpoch: 1451 Meta-valid_Accuracy: 0.6940\nEpoch: 1500 loss_train: 0.8931 acc_train: 0.9600\nEpoch: 1501 Meta-valid_Accuracy: 0.7532\nEpoch: 1551 Meta-valid_Accuracy: 0.7316\nEpoch: 1601 Meta-valid_Accuracy: 0.7060\nEpoch: 1651 Meta-valid_Accuracy: 0.6964\nEpoch: 1701 Meta-valid_Accuracy: 0.7260\nEpoch: 1750 loss_train: 0.8787 acc_train: 0.9400\nEpoch: 1751 Meta-valid_Accuracy: 0.7308\nEpoch: 1801 Meta-valid_Accuracy: 0.7196\nEpoch: 1851 Meta-valid_Accuracy: 0.7148\nEpoch: 1901 Meta-valid_Accuracy: 0.7048\nEpoch: 1951 Meta-valid_Accuracy: 0.7112\nEpoch: 2000 loss_train: 1.1547 acc_train: 0.9600\nTest Acc 0.6988000000000001\nbegin  cora-full n=  5 k=  5\nEpoch: 0051 Meta-valid_Accuracy: 0.6860\nEpoch: 0101 Meta-valid_Accuracy: 0.6936\nEpoch: 0151 Meta-valid_Accuracy: 0.6764\nEpoch: 0201 Meta-valid_Accuracy: 0.7096\nEpoch: 0250 loss_train: 0.9338 acc_train: 1.0000\nEpoch: 0251 Meta-valid_Accuracy: 0.7048\nEpoch: 0301 Meta-valid_Accuracy: 0.7116\nEpoch: 0351 Meta-valid_Accuracy: 0.6964\nEpoch: 0401 Meta-valid_Accuracy: 0.7308\nEpoch: 0451 Meta-valid_Accuracy: 0.7244\nEpoch: 0500 loss_train: 0.9141 acc_train: 1.0000\nEpoch: 0501 Meta-valid_Accuracy: 0.7156\nEpoch: 0551 Meta-valid_Accuracy: 0.7004\nEpoch: 0601 Meta-valid_Accuracy: 0.7156\nEpoch: 0651 Meta-valid_Accuracy: 0.7076\nEpoch: 0701 Meta-valid_Accuracy: 0.6924\nEpoch: 0750 loss_train: 0.9978 acc_train: 0.9600\nEpoch: 0751 Meta-valid_Accuracy: 0.7196\nEpoch: 0801 Meta-valid_Accuracy: 0.7068\nEpoch: 0851 Meta-valid_Accuracy: 0.7192\nEpoch: 0901 Meta-valid_Accuracy: 0.6964\nTest Acc 0.6799999999999998\nbegin  cora-full n=  5 k=  5\nEpoch: 0051 Meta-valid_Accuracy: 0.6792\nEpoch: 0101 Meta-valid_Accuracy: 0.7044\nEpoch: 0151 Meta-valid_Accuracy: 0.6748\nEpoch: 0201 Meta-valid_Accuracy: 0.6936\nEpoch: 0250 loss_train: 1.3099 acc_train: 0.9600\nEpoch: 0251 Meta-valid_Accuracy: 0.7008\nEpoch: 0301 Meta-valid_Accuracy: 0.7076\nEpoch: 0351 Meta-valid_Accuracy: 0.6840\nEpoch: 0401 Meta-valid_Accuracy: 0.7216\nEpoch: 0451 Meta-valid_Accuracy: 0.7248\nEpoch: 0500 loss_train: 1.3140 acc_train: 0.9800\nEpoch: 0501 Meta-valid_Accuracy: 0.7048\nEpoch: 0551 Meta-valid_Accuracy: 0.7160\nEpoch: 0601 Meta-valid_Accuracy: 0.6828\nEpoch: 0651 Meta-valid_Accuracy: 0.6912\nEpoch: 0701 Meta-valid_Accuracy: 0.7352\nEpoch: 0750 loss_train: 0.9998 acc_train: 0.9600\nEpoch: 0751 Meta-valid_Accuracy: 0.6932\nEpoch: 0801 Meta-valid_Accuracy: 0.7060\nEpoch: 0851 Meta-valid_Accuracy: 0.7092\nEpoch: 0901 Meta-valid_Accuracy: 0.6888\nEpoch: 0951 Meta-valid_Accuracy: 0.6900\nEpoch: 1000 loss_train: 1.1029 acc_train: 1.0000\nEpoch: 1001 Meta-valid_Accuracy: 0.6948\nEpoch: 1051 Meta-valid_Accuracy: 0.7148\nEpoch: 1101 Meta-valid_Accuracy: 0.7096\nEpoch: 1151 Meta-valid_Accuracy: 0.7300\nEpoch: 1201 Meta-valid_Accuracy: 0.7296\nTest Acc 0.7075999999999999\nbegin  cora-full n=  5 k=  5\nEpoch: 0051 Meta-valid_Accuracy: 0.7144\nEpoch: 0101 Meta-valid_Accuracy: 0.6792\nEpoch: 0151 Meta-valid_Accuracy: 0.7132\nEpoch: 0201 Meta-valid_Accuracy: 0.7112\nEpoch: 0250 loss_train: 1.1511 acc_train: 0.9400\nEpoch: 0251 Meta-valid_Accuracy: 0.6952\nEpoch: 0301 Meta-valid_Accuracy: 0.7084\nEpoch: 0351 Meta-valid_Accuracy: 0.6848\nEpoch: 0401 Meta-valid_Accuracy: 0.7168\nEpoch: 0451 Meta-valid_Accuracy: 0.7068\nEpoch: 0500 loss_train: 1.0542 acc_train: 0.9600\nEpoch: 0501 Meta-valid_Accuracy: 0.6944\nEpoch: 0551 Meta-valid_Accuracy: 0.7072\nEpoch: 0601 Meta-valid_Accuracy: 0.7368\nEpoch: 0651 Meta-valid_Accuracy: 0.7008\nEpoch: 0701 Meta-valid_Accuracy: 0.7228\nEpoch: 0750 loss_train: 1.2131 acc_train: 0.9600\nEpoch: 0751 Meta-valid_Accuracy: 0.7188\nEpoch: 0801 Meta-valid_Accuracy: 0.7048\nEpoch: 0851 Meta-valid_Accuracy: 0.6916\nEpoch: 0901 Meta-valid_Accuracy: 0.7088\nEpoch: 0951 Meta-valid_Accuracy: 0.7172\nEpoch: 1000 loss_train: 1.2006 acc_train: 0.9000\nEpoch: 1001 Meta-valid_Accuracy: 0.6980\nEpoch: 1051 Meta-valid_Accuracy: 0.7000\nEpoch: 1101 Meta-valid_Accuracy: 0.7032\nTest Acc 0.7119999999999997\nbegin  cora-full n=  10 k=  3\nEpoch: 0051 Meta-valid_Accuracy: 0.4806\nEpoch: 0101 Meta-valid_Accuracy: 0.5184\nEpoch: 0151 Meta-valid_Accuracy: 0.5074\nEpoch: 0201 Meta-valid_Accuracy: 0.5012\nEpoch: 0250 loss_train: 1.5440 acc_train: 0.9400\nEpoch: 0251 Meta-valid_Accuracy: 0.5304\nEpoch: 0301 Meta-valid_Accuracy: 0.5184\nEpoch: 0351 Meta-valid_Accuracy: 0.5062\nEpoch: 0401 Meta-valid_Accuracy: 0.5346\nEpoch: 0451 Meta-valid_Accuracy: 0.5272\nEpoch: 0500 loss_train: 1.4244 acc_train: 0.9900\nEpoch: 0501 Meta-valid_Accuracy: 0.5490\nEpoch: 0551 Meta-valid_Accuracy: 0.5418\nEpoch: 0601 Meta-valid_Accuracy: 0.5312\nEpoch: 0651 Meta-valid_Accuracy: 0.5170\nEpoch: 0701 Meta-valid_Accuracy: 0.5316\nEpoch: 0750 loss_train: 1.4837 acc_train: 0.9000\nEpoch: 0751 Meta-valid_Accuracy: 0.5308\nEpoch: 0801 Meta-valid_Accuracy: 0.5382\nEpoch: 0851 Meta-valid_Accuracy: 0.5444\nEpoch: 0901 Meta-valid_Accuracy: 0.5530\nEpoch: 0951 Meta-valid_Accuracy: 0.5208\nEpoch: 1000 loss_train: 1.5197 acc_train: 0.9800\nEpoch: 1001 Meta-valid_Accuracy: 0.5246\nEpoch: 1051 Meta-valid_Accuracy: 0.5364\nEpoch: 1101 Meta-valid_Accuracy: 0.5346\nEpoch: 1151 Meta-valid_Accuracy: 0.5292\nEpoch: 1201 Meta-valid_Accuracy: 0.5358\nEpoch: 1250 loss_train: 1.4206 acc_train: 0.9600\nEpoch: 1251 Meta-valid_Accuracy: 0.5354\nEpoch: 1301 Meta-valid_Accuracy: 0.5526\nEpoch: 1351 Meta-valid_Accuracy: 0.5220\nEpoch: 1401 Meta-valid_Accuracy: 0.5180\nTest Acc 0.4976\nbegin  cora-full n=  10 k=  3\n","output_type":"stream"}]}]}